{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b6774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae103b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img=tf.keras.utils.load_img(r\"C:/Users/user/Desktop/cnn/train/cedrick/IMG_0625.JPG\")\n",
    "X=tf.keras.utils.img_to_array(img)\n",
    "X=X.reshape((1,) + X.shape)\n",
    "\n",
    "\n",
    "i=0\n",
    "for batch in datagen.flow(X, batch_size=1,\n",
    "                          save_to_dir=r'C:\\Users\\user\\Desktop\\cnn\\preview', save_prefix='ced', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5e830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set=datagen.flow_from_directory(r'C:\\Users\\user\\Desktop\\cnn\\preview',target_size=(64,64),batch_size=32,class_mode='binary',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd2e1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329b3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c725631c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'load_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/user/Desktop/cnn/train/cedrick/IMG_0625.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "img=image.load_img(r\"C:/Users/user/Desktop/cnn/train/cedrick/IMG_0625.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089a0330",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PLT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPLT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PLT'"
     ]
    }
   ],
   "source": [
    "from PLT import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02a719c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2307985",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array([a**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fed1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'number':[2,3,5,10,7],'num':[4,9,25,100,47]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46d13395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd54b179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number  num\n",
       "0       2    4\n",
       "1       3    9\n",
       "2       5   25\n",
       "3      10  100\n",
       "4       7   47"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb40b730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='number', ylabel='num'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAP9ElEQVR4nO3df6xfdX3H8edLKoHiHCA3TQdsxUBcGHOD3aGLxjnZDNctQiZjuEU7Q9ZkUy8Ol8H0DzIXE53OH02MSydsdXFMhk7YQqcEIc5s4G6ByY/q6PihNEAvU/DHjTrkvT++h08v7a182/L9nm/7fT6Sm+/52b44CX3dc873fE6qCkmSAJ7TdwBJ0uSwFCRJjaUgSWosBUlSYylIkppVfQc4EMcdd1ytW7eu7xiSdFDZunXro1U1s9K6g7oU1q1bx8LCQt8xJOmgkuSBva3z8pEkqbEUJEmNpSBJaiwFSVJjKUiSmpGVQpIrkuxMcueyZccmuT7JPd3nMd3yJNmYZHuSLyc5Y1S5JEl7N8ozhb8Fzt5t2aXADVV1CnBDNw8wB5zS/WwAPjrCXJKkvRjZcwpV9YUk63ZbfA7wym56M3ATcEm3/OM1GMf75iRHJ1lbVQ+NKp8kAWzcuJEtW7bs0z5LS0uM87UDSVi9evU+7TM3N8f8/Pw+/13jvqewZtk/9A8Da7rp44GvL9vuwW7ZHpJsSLKQZGFxcXF0SSVpCvX2RHNVVZJ9rtqq2gRsApidnfUNQZIOyPz8/H79Rn2oGveZwiNJ1gJ0nzu75TuAE5dtd0K3TJI0RuMuhWuB9d30euCaZcvf2H0L6aXA495PkKTxG9nloyRXMripfFySB4HLgPcAVyW5EHgAOL/b/DrgNcB2YAl406hySZL2bpTfPnr9XladtcK2Bbx5VFkkScPxiWZJUmMpSJIaS0GS1FgKkqTGUpAkNZaCJKmxFCRJjaUgSWosBUlSYylIkhpLQZLUWAqSpMZSkCQ1loIkqbEUJEmNpSBJaiwFSVJjKUiSGktBktRYCpKkxlKQJDWWgiSpsRQkSY2lIElqLAVJUmMpSJIaS0GS1FgKkqTGUpAkNZaCJKmxFCRJjaUgSWp6KYUkf5TkriR3JrkyyRFJTkpyS5LtST6Z5PA+sknSNBt7KSQ5HpgHZqvqNOAw4ALgvcAHq+pk4JvAhePOJknTrq/LR6uAI5OsAlYDDwGvAq7u1m8Gzu0nmiRNr7GXQlXtAN4PfI1BGTwObAUeq6onus0eBI5faf8kG5IsJFlYXFwcR2RJmhp9XD46BjgHOAn4CeAo4Oxh96+qTVU1W1WzMzMzI0opSdOpj8tHvwrcV1WLVfV/wKeBlwFHd5eTAE4AdvSQTZKmWh+l8DXgpUlWJwlwFnA3cCNwXrfNeuCaHrJJ0lTr457CLQxuKN8K3NFl2ARcAlycZDvwAuDycWeTpGm36pk3efZV1WXAZbstvhc4s4c4kqSOTzRLkhpLQZLUWAqSpMZSkCQ1loIkqbEUJEmNpSBJaiwFSVJjKUiSGktBktRYCpKkxlKQJDWWgiSpsRQkSY2lIElqLAVJUmMpSJIaS0GS1FgKkqTGUpAkNZaCJKmxFCRJjaUgSWosBUlSYylIkhpLQZLUWAqSpMZSkCQ1loIkqbEUJEmNpSBJaiwFSVLTSykkOTrJ1Um+kmRbkl9KcmyS65Pc030e00c2SZpmfZ0pfBj416r6aeDngG3ApcANVXUKcEM3L0kao7GXQpIfB14BXA5QVT+oqseAc4DN3WabgXPHnU2Spl0fZwonAYvA3yS5LcnHkhwFrKmqh7ptHgbW9JBNkqZaH6WwCjgD+GhVnQ58l90uFVVVAbXSzkk2JFlIsrC4uDjysJI0TfoohQeBB6vqlm7+agYl8UiStQDd586Vdq6qTVU1W1WzMzMzYwksSdNi7KVQVQ8DX0/yom7RWcDdwLXA+m7ZeuCacWeTpGm3qqe/963AJ5IcDtwLvIlBQV2V5ELgAeD8nrJJ0tTqpRSq6nZgdoVVZ405iiRpGZ9oliQ1loIkqbEUJEnNUPcUkhwNvBFYt3yfqpofSSpJUi+GvdF8HXAzcAfw5OjiSJL6NGwpHFFVF480iSSpd8PeU/i7JL+fZG03xPWxSY4daTJJ0tgNe6bwA+B9wDvZNSZRAS8cRShJUj+GLYW3AydX1aOjDCNJ6tewl4+2A0ujDCJJ6t+wZwrfBW5PciPw/acW+pVUSTq0DFsKn+l+JEmHsKFKoao2P/NWkqSD3bBPNN/HCm9Cqyq/fSRJh5BhLx8tH+b6COC3AJ9TkKRDzFDfPqqq/132s6OqPgT8+mijSZLGbdjLR2csm30OgzOHvt7aJkkakWH/Yf9Ldt1TeAK4n8ElJEnSIWTYUpgDXsfTh86+AHjXCDJJknqyL88pPAbcCnxvVGEkSf0athROqKqzR5pE0ths3LiRLVu27NM+S0tLVO3xzfSRScLq1av3aZ+5uTnm5x1o4UAMO/bRvyf52ZEmkST1LsM0f5K7gZOB+xiMfRSgqurFo433o83OztbCwkKfESTpoJNka1XNrrRuX240S5IOccOOffTAqINIkvo37D0FSdIUsBQkSY2lIElqLAVJUmMpSJIaS0GS1FgKkqTGUpAkNb2VQpLDktyW5F+6+ZOS3JJke5JPJjm8r2ySNK36PFO4CNi2bP69wAer6mTgm8CFvaSSpCnWSykkOYHBO54/1s0HeBVwdbfJZuDcPrJJ0jTr60zhQ8CfAE928y8AHquqJ7r5B4HjV9oxyYYkC0kWFhcXRx5UkqbJ2EshyW8AO6tq6/7sX1Wbqmq2qmZnZmae5XSSNN2GHTr72fQy4LVJXgMcATwf+DBwdJJV3dnCCcCOHrJJ0lQb+5lCVf1pVZ1QVeuAC4DPV9XvAjcC53WbrQeuGXc2SZp2k/ScwiXAxUm2M7jHcHnPeSRp6vRx+aipqpuAm7rpe4Ez+8wjSdNuks4UJEk9sxQkSY2lIElqLAVJUmMpSJIaS0GS1FgKkqTGUpAkNZaCJKmxFCRJjaUgSWosBUlSYylIkhpLQZLUWAqSpMZSkCQ1loIkqbEUJEmNpSBJaiwFSVJjKUiSGktBktRYCpKkxlKQJDWr+g4gjcvGjRvZsmXLPu2ztLREVY0o0Z6SsHr16n3aZ25ujvn5+REl0rTxTEGS1GScvwU922ZnZ2thYaHvGJJ0UEmytapmV1rnmYIkqbEUJEmNpSBJaiwFSVJjKUiSmrGXQpITk9yY5O4kdyW5qFt+bJLrk9zTfR4z7mySNO36OFN4Anh7VZ0KvBR4c5JTgUuBG6rqFOCGbl6SNEZjL4Wqeqiqbu2mvw1sA44HzgE2d5ttBs4ddzZJmna93lNIsg44HbgFWFNVD3WrHgbW7GWfDUkWkiwsLi6OJ6gkTYneSiHJ84BPAW+rqm8tX1eDx6xXfNS6qjZV1WxVzc7MzIwhqSRNj15KIclzGRTCJ6rq093iR5Ks7davBXb2kU2Splkf3z4KcDmwrao+sGzVtcD6bno9cM24s0nStOtj6OyXAW8A7khye7fsHcB7gKuSXAg8AJzfQzZJmmpjL4Wq+iKQvaw+a5xZJElP5xPNkqTGUpAkNZaCJKmxFCRJjaUgSWosBUlSYylIkhpLQZLUWAqSpMZSkCQ1loIkqbEUJEmNpSBJaiwFSVJjKUiSGktBktT08eY1jdHGjRvZsmXLPu2ztLREVY0o0Z6SsHr16n3aZ25ujvn5+RElkqaXZwqSpCbj/I3w2TY7O1sLCwt9x5Ckg0qSrVU1u9I6zxQkSY2lIElqLAVJUmMpSJIaS0GS1FgKkqTGUpAkNYfkE80+xStJ+8czBUlS4xPNkjRlfKJZkjQUS0GS1FgKkqRmokohydlJvppke5JL+84jSdNmYkohyWHAR4A54FTg9UlO7TeVJE2XiSkF4Exge1XdW1U/AP4BOKfnTJI0VSapFI4Hvr5s/sFu2dMk2ZBkIcnC4uLi2MJJ0jSYpFIYSlVtqqrZqpqdmZnpO44kHVImaZiLHcCJy+ZP6Jbt1datWx9N8sBIUw3nOODRvkNMCI/FgMdhF4/FLpNyLH5qbysm5onmJKuA/wbOYlAG/wn8TlXd1WuwISRZ2NvTgdPGYzHgcdjFY7HLwXAsJuZMoaqeSPIW4LPAYcAVB0MhSNKhZGJKAaCqrgOu6zuHJE2rg+5G84Ta1HeACeKxGPA47OKx2GXij8XE3FOQJPXPMwVJUmMpSJIaS2E/JTkxyY1J7k5yV5KL+s7UlyRHJPlSkv/qjsWf9Z2pT0nuT3JHktuTTO1boJK8qDsGT/18K8nb+s41LkmuSLIzyZ3Llh2b5Pok93Sfx/SZcSXeU9hPSdYCa6vq1iQ/BmwFzq2qu3uONnZJAhxVVd9J8lzgi8BFVXVzz9F6keR+YLaqJuEhpYnQDXi5A3hJVU3CA6cjl+QVwHeAj1fVad2yvwC+UVXv6UaCPqaqLukz5+48U9hPVfVQVd3aTX8b2MYKYzVNgxr4Tjf73O7H3za03FnA/0xLIQBU1ReAb+y2+Bxgcze9GTh3nJmGYSk8C5KsA04Hbuk5Sm+SHJbkdmAncH1VTe2xYFCIn0uyNcmGvsNMiAuAK/sOMQHWVNVD3fTDwJo+w6zEUjhASZ4HfAp4W1V9q+88famqH1bVzzMYs+rMJKf1HKlPL6+qMxi8G+TN3WWEqZXkcOC1wD/2nWWS1ODa/cSdUVsKB6C7fv4p4BNV9em+80yCqnoMuBE4u+covamqHd3nTuCfGLwrZJrNAbdW1SN9B5kAj3T3I5+6L7mz5zx7sBT2U3dz9XJgW1V9oO88fUoyk+TobvpI4NeAr/QaqidJjuq+eECSo4BXA3f+6L0Oea/HS0dPuRZY302vB67pMcuK/PbRfkrycuDfgDuAJ7vF7+jGb5oqSV7M4KbZYQx+0biqqt7Vb6p+JHkhg7MDGIwt9vdV9e4eI/WqK8avAS+sqsf7zjNOSa4EXslguOxHgMuAzwBXAT8JPACcX1W734zulaUgSWq8fCRJaiwFSVJjKUiSGktBktRYCpKkxlKQxiDJTUkm+oXtElgK0sRLMlHvUtehzVKQlkmyLsm2JH/dvRvic0mOXP6bfpLjuuGxSfJ7ST7TjY1/f5K3JLk4yW1Jbk5y7LI//g3dewXuTHJmt/9R3bj7X+r2OWfZn3ttks8DN4z5MGiKWQrSnk4BPlJVPwM8BrzuGbY/DfhN4BeBdwNLVXU68B/AG5dtt7obNPAPgSu6Ze8EPl9VZwK/AryvewoY4AzgvKr65QP+L5KG5GmptKf7qur2bnorsO4Ztr+xe6fGt5M8Dvxzt/wO4MXLtrsSBuPsJ3l+N17Uq4HXJvnjbpsjGAyBAIMhyCdqCAQd+iwFaU/fXzb9Q+BI4Al2nVkf8SO2f3LZ/JM8/f+x3ceUKSDA66rqq8tXJHkJ8N19Ti4dIC8fScO5H/iFbvq8/fwzfhvaYIqPdwPEfRZ4azfqLklOP8Cc0gGxFKThvB/4gyS3MRj1cn98r9v/r4ALu2V/zuD1pV9Oclc3L/XGUVIlSY1nCpKkxlKQJDWWgiSpsRQkSY2lIElqLAVJUmMpSJKa/wdhIvohoy7BrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=\"number\",y=\"num\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51674944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# pip install -U spacy-lookups-data\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e013583",
   "metadata": {},
   "source": [
    "## In this article, we are going to perform the below tasks.\n",
    "## General Feature Extraction\n",
    "File loading \\n\n",
    "Word counts\n",
    "Characters count\n",
    "Average characters per word\n",
    "Stop words count\n",
    "Count #HashTags and @Mentions\n",
    "If numeric digits are present in twitts\n",
    "Upper case word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b1338",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing and Cleaning\n",
    "Lower case\n",
    "Contraction to Expansion\n",
    "Emails removal and counts\n",
    "URLs removal and counts\n",
    "Removal of RT\n",
    "Removal of Special Characters\n",
    "Removal of multiple spaces\n",
    "Removal of HTML tags\n",
    "Removal of accented characters\n",
    "Removal of Stop Words\n",
    "Conversion into base form of words\n",
    "Common Occuring words Removal\n",
    "Rare Occuring words Removal\n",
    "Word Cloud\n",
    "Spelling Correction\n",
    "Tokenization\n",
    "Lemmatization\n",
    "Detecting Entities using NER\n",
    "Noun Detection\n",
    "Language Detection\n",
    "Sentence Translation\n",
    "Using Inbuilt Sentiment Classifier\n",
    "Advanced Text Processing and Feature Extraction\n",
    "N-Gram, Bi-Gram etc\n",
    "Bag of Words (BoW)\n",
    "Term Frequency Calculation TF\n",
    "Inverse Document Frequency IDF\n",
    "TFIDF Term Frequency - Inverse Document Frequency\n",
    "Word Embedding Word2Vec using SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae464d0",
   "metadata": {},
   "source": [
    "## Machine Learning Models for Text Classification\n",
    "SGDClassifier\n",
    "LogisticRegression\n",
    "LogisticRegressionCV\n",
    "LinearSVC\n",
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9236aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter16m.csv', encoding = 'latin1', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559751a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21161fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[5, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee4753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ad8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_map = {0: 'negative', 4: 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59865a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_counts'] = df['twitts'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6992d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_counts'] = df['twitts'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37bfdf",
   "metadata": {},
   "source": [
    "Average Word Length\n",
    "In this step, we have created a function get_avg_word_len() in which we are calculating the average word length inside each sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_avg_word_len(x):\n",
    "    words = x.split()\n",
    "    word_len = 0\n",
    "    for word in words:\n",
    "        word_len = word_len + len(word)\n",
    "    return word_len/len(words) # != len(x)/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7da147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word_len'] = df['twitts'].apply(lambda x: get_avg_word_len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('this is nlp lesson')/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00134caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645d8ee",
   "metadata": {},
   "source": [
    "## Stop Words Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'this is text data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a99398",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([t for t in x.split() if t in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stop_words_len'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5275ee19",
   "metadata": {},
   "source": [
    "##  Count #HashTags and @Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa38f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'this #hashtag and this is @mention'\n",
    "# x = x.split()\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
    "df['mentions_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ddde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc40774",
   "metadata": {},
   "source": [
    "## If numeric digits are present in twitts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numerics_count'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d105c",
   "metadata": {},
   "source": [
    "## UPPER case words count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upper_counts'] = df['twitts'].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910edd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[96]['twitts']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5cb12c",
   "metadata": {},
   "source": [
    "##  Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79ddc6",
   "metadata": {},
   "source": [
    "## Contraction to Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"i don't know what you want, can't, he'll, i'd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20949c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key, value)\n",
    "        return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"hi, i'd be happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_to_exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff9bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['twitts'] = df['twitts'].apply(lambda x: cont_to_exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e8a66",
   "metadata": {},
   "source": [
    "## Count and Remove Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'hi my email me at email@email.com another@email.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emails'] = df['twitts'].apply(lambda x: re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0835a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emails_count'] = df['emails'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b266468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['emails_count']>0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0775bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ee3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'hi my email me at  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: re.sub(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd33f7",
   "metadata": {},
   "source": [
    "## Count URLs and Remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e872ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'hi, to watch more visit https://youtube.com/kgptalkie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('https', 'youtube.com', '/kgptalkie')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497eec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['urls_flag'] = df['twitts'].apply(lambda x: len(re.findall(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadb2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99c8a3",
   "metadata": {},
   "source": [
    "## Remove RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: re.sub('RT', \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df9468",
   "metadata": {},
   "source": [
    "## Special Chars removal or punctuation remova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1549748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f014b",
   "metadata": {},
   "source": [
    "##  Remove multiple spaces \"hi hello \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcaad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'thanks    for    watching and    please    like this video'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd93d8",
   "metadata": {},
   "source": [
    "## Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43699de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5207f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '<html><h2>Thanks for watching</h2></html>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f10ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup(x, 'lxml').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['twitts'] = df['twitts'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988ed9d",
   "metadata": {},
   "source": [
    "## Remove Accented Chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9561cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'Áccěntěd těxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(x):\n",
    "    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_accented_chars(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8cc9c",
   "metadata": {},
   "source": [
    "## SpaCy and NLP\n",
    "## Remove Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fe035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96000dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'this is stop words removal code is a the an how what'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84045820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([t for t in x.split() if t not in STOP_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d034abf",
   "metadata": {},
   "source": [
    "## Convert into base or root form of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'kenichan dived times ball managed save 50 rest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10371a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_base(x):\n",
    "    x_list = []\n",
    "    doc = nlp(x)\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma = str(token.lemma_)\n",
    "        if lemma == '-PRON-' or lemma == 'be':\n",
    "            lemma = token.text\n",
    "        x_list.append(lemma)\n",
    "    print(\" \".join(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_to_base(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e87bd",
   "metadata": {},
   "source": [
    "##  Common words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(df.head()['twitts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_comm = pd.Series(text).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f20 = freq_comm[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c67980",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['twitts'] = df['twitts'].apply(lambda x: \" \".join([t for t in x.split() if t not in f20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444828f9",
   "metadata": {},
   "source": [
    "## Rare words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ef38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare20 = freq_comm[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac92fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = freq_comm[freq_comm.values == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb831da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitts'] = df['twitts'].apply(lambda x: ' '.join([t for t in x.split() if t not in rare20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6a974",
   "metadata": {},
   "source": [
    "## Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb49ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2832a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ' '.join(text[:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width = 800, height=400).generate(x)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533ab0a",
   "metadata": {},
   "source": [
    "## Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4200f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'tanks forr waching this vidio carri'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd11108",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TextBlob(x).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(\"tanks for watching this video carry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ade3ab9",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'thanks#watching this video. please like it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "WordList(['thanks', 'watching', 'this', 'video', 'please', 'like', 'it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05251a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(x)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106904c",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'runs run running ran'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143853bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in x.split():\n",
    "    print(Word(token).lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c599df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(x)\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e52c7",
   "metadata": {},
   "source": [
    "## Detect Entities using NER of SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000699",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Breaking News: Donald Trump, the president of the USA is looking to sign a deal to mine the moon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f86e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(x)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65594e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style = 'ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc14226",
   "metadata": {},
   "source": [
    "## dectecting nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c1b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for noun in doc.noun_chunks:\n",
    "    print(noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd742385",
   "metadata": {},
   "source": [
    "## Translation and Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b567a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Breaking News: Donald Trump, the president of the USA is looking to sign a deal to mine the moon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TextBlob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f52450",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d590a",
   "metadata": {},
   "source": [
    "## Advanced Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371b166",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'thanks for watching'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TextBlob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58581c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[WordList(['thanks', 'for', 'watching'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714782e",
   "metadata": {},
   "source": [
    "## Bag of Words BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640af642",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['this is first sentence this is', 'this is second', 'this is last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1))\n",
    "text_counts = cv.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83156dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5700aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7640734",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = pd.DataFrame(text_counts.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c004de8",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb161b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=['this is first sentence this is', 'this is second', 'this is last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = bow.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in enumerate(tf.iterrows()):\n",
    "    for col in row[1].index:\n",
    "        tf.loc[index, col] = tf.loc[index, col]/sum(row[1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971db4f8",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(x, columns=['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d413a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = bow.astype('bool')\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeff0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb['is'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d765d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = bb.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = []\n",
    "for col in cols:\n",
    "    nz.append(bb[col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ab126",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = []\n",
    "for index, col in enumerate(cols):\n",
    "    idf.append(np.log((N + 1)/(nz[index] + 1)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d561fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "x_tfidf = tfidf.fit_transform(x_df['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f295a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940990ed",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ab1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp('thank you! dog cat lion dfasaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.has_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ccf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "token.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('cat').vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58dc668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling the number of rows\n",
    "\n",
    "df0 = df[df['sentiment']==0].sample(2000)\n",
    "df4 = df[df['sentiment']==4].sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9931218",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = df0.append(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c376b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the twitts,sentiment and emails columns\n",
    "\n",
    "dfr_feat = dfr.drop(labels=['twitts','sentiment','emails'], axis = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10567dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfr['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ed517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e76d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_bow = pd.DataFrame(text_counts.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr_bow.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4da8f",
   "metadata": {},
   "source": [
    "## ML Algorithms\n",
    "## Importing Libraries for ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22664add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbccf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(n_jobs=-1, random_state=42, max_iter=200)\n",
    "lgr = LogisticRegression(random_state=42, max_iter=200)\n",
    "lgrcv = LogisticRegressionCV(cv = 2, random_state=42, max_iter=1000)\n",
    "svm = LinearSVC(random_state=42, max_iter=200)\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = {'SGD': sgd, 'LGR': lgr, 'LGR-CV': lgrcv, 'SVM': svm, 'RFC': rfc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ab7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we are training our model by defining the function classify.\n",
    "\n",
    "def classify(X, y):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "    \n",
    "    for key in clf.keys():\n",
    "        clf[key].fit(X_train, y_train)\n",
    "        y_pred = clf[key].predict(X_test)\n",
    "        ac = accuracy_score(y_test, y_pred)\n",
    "        print(key, \" ---> \", ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classify(dfr_bow, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0694c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af20188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414ca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88ef1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617c828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
